{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50eed29-a88d-4738-a774-66325e52603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/repo/learning/false' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install libraries quietly\n",
    "!pip install --quiet datasets transformers pytorch-lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82662f43-bcc6-4d5a-9d4c-0f2f5992ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcoffeedrunk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import ViTFeatureExtractor\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# Data Stuff\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Model\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Weights and Biases\n",
    "# https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022-labs/blob/main/lab04/training/run_experiment.py\n",
    "\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e60952-0950-4434-b3e6-e4a3b87c657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset beans (/root/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bb1f992b524d509590f0ad5e28e833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Stuff\n",
    "\n",
    "\n",
    "batch_size= 8\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    inputs['image'] = example_batch['image']\n",
    "    return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch]),\n",
    "        'image': [x['image'] for x in batch]\n",
    "    }\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset('beans')\n",
    "prepared_ds = ds.with_transform(transform)\n",
    "\n",
    "labels_names = ds['train'].features['labels'].names\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(prepared_ds[\"train\"],\n",
    "                      batch_size=batch_size,\n",
    "                      collate_fn=collate_fn)\n",
    "\n",
    "val_dataloader = DataLoader(prepared_ds[\"validation\"],\n",
    "                      batch_size=batch_size,\n",
    "                      collate_fn=collate_fn)\n",
    "\n",
    "test_dataloader = DataLoader(prepared_ds[\"test\"],\n",
    "                      batch_size=batch_size,\n",
    "                      collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38c4fa1-1134-4e67-ab40-f29a12e331d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_file_path', 'image', 'labels'],\n",
       "    num_rows: 133\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4cd97a-708b-4e51-ab14-4e89f592e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class Vit(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTForImageClassification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_labels=3, lr = 2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "        self.model = ViTForImageClassification.from_pretrained(self.model_name_or_path,\n",
    "                                                                num_labels = num_labels)\n",
    "\n",
    "        # log hyperparameters\n",
    "        # https://www.youtube.com/watch?v=hUXQm46TAKc&list=PLD80i8An1OEGajeVo15ohAQYF1Ttle0lk&index=4\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Accuracy\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x ):\n",
    "        pixel_values = x[\"pixel_values\"]\n",
    "        labels = x[\"labels\"]\n",
    "        \n",
    "        outs = self.model(pixel_values = pixel_values, labels=labels)\n",
    "\n",
    "        loss = outs.loss\n",
    "        logits = outs.logits\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # self.hparams comes from self.save_hyperparameters()\n",
    "        return AdamW(self.parameters(), lr=self.hparams[\"lr\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/vit#transformers.ViTForImageClassification.forward\n",
    "        # loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided)\n",
    "\n",
    "        loss, logits = self(batch)\n",
    "\n",
    "        self.train_acc(logits, batch[\"labels\"])\n",
    "\n",
    "        self.log(\"train/loss\", loss,  on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/acc\", self.train_acc,  on_epoch=True, prog_bar=True)\n",
    "        outputs = {\"loss\": loss}\n",
    "        return outputs\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, logits = self(batch)\n",
    "\n",
    "        self.val_acc(logits, batch[\"labels\"])\n",
    "\n",
    "        self.log(\"validation/loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"validation/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        outputs = {\"loss\": loss,\n",
    "                   \"images\": batch[\"image\"],\n",
    "                   \"pred\": torch.argmax(logits, 1),\n",
    "                   \"label\":batch[\"labels\"]}\n",
    "        return outputs\n",
    "\n",
    "    # TODO: arrumar esse pedaÃ§o aqui e colocar o out do validation_step como os preds ou logits\n",
    "#     def validation_epoch_end(self, validation_step_outputs):\n",
    "        \n",
    "#         # Change the hardcoded size\n",
    "#         print(\"##epoch end\", validation_step_outputs)\n",
    "        \n",
    "#         # dummy_input = torch.zeros([3,224,224], device=self.device)\n",
    "#         # model_filename = f\"model_{str(self.global_step).zfill(5)}.onnx\"\n",
    "#         # torch.onnx.export(self, dummy_input, model_filename, opset_version=11)\n",
    "#         # artifact = wandb.Artifact(name=\"model.ckpt\", type=\"model\")\n",
    "#         # artifact.add_file(model_filename)\n",
    "#         # self.logger.experiment.log_artifact(artifact)\n",
    "\n",
    "#         flattened_logits = torch.flatten(torch.cat(validation_step_outputs))\n",
    "#         self.logger.experiment.log(\n",
    "#             {\"valid/logits\": wandb.Histogram(flattened_logits.to(\"cpu\")),\n",
    "#             \"global_step\": self.global_step})\n",
    "\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         loss, logits = self(pixel_values = batch[\"pixel_values\"],\n",
    "#                 labels=batch[\"labels\"])\n",
    "\n",
    "#         self.test_acc(logits, batch[\"labels\"])\n",
    "\n",
    "#         self.log(\"test/loss_epoch\", loss, on_step=False, on_epoch=True)\n",
    "#         self.log(\"test/acc_epoch\", self.test_acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    # def test_epoch_end(self, test_step_outputs):  # args are defined as part of pl API\n",
    "    #     # Change the hardcoded size\n",
    "    #     dummy_input = torch.zeros([3,224,224], device=self.device)\n",
    "    #     model_filename = \"model_final.onnx\"\n",
    "    #     self.to_onnx(model_filename, dummy_input, export_params=True)\n",
    "    #     artifact = wandb.Artifact(name=\"model.ckpt\", type=\"model\")\n",
    "    #     artifact.add_file(model_filename)\n",
    "    #     wandb.log_artifact(artifact)\n",
    "\n",
    "\n",
    "model = Vit(num_labels = len(labels_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed8af9ec-7453-425a-93b3-85608cc1abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePredictionLogger(pl.Callback):\n",
    "    def __init__(self, num_samples=1):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "#         val_images = batch[\"image\"][:self.num_samples]\n",
    "#         val_labels = batch[\"labels\"][:self.num_samples]\n",
    "#         val_tensors = batch[\"pixel_values\"][:self.num_samples]\n",
    "        \n",
    "#         # print(val_tensors[:2])\n",
    "#         inputs = {\"pixel_values\":val_tensors, \"labels\":val_labels}\n",
    "#         _, logits = pl_module(inputs)\n",
    "#         preds = torch.argmax(logits, 1)\n",
    "\n",
    "        val_images = outputs[\"images\"][:1]\n",
    "        val_labels = outputs[\"label\"][:1]\n",
    "        val_preds = outputs[\"pred\"][:1]\n",
    "        \n",
    "        \n",
    "        # print(outputs)\n",
    "        trainer.logger.experiment.log({\n",
    "            \"examples\": [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\")\n",
    "                            for x, pred, y in zip(val_images, val_preds, val_labels)],\n",
    "            \"global_step\": trainer.global_step})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f307f8c3-fba3-450a-9386-0a30e1868276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20221114_215133-1q96tylh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/coffeedrunk/pytorch-lightning/runs/1q96tylh\" target=\"_blank\">peach-cherry-20</a></strong> to <a href=\"https://wandb.ai/coffeedrunk/pytorch-lightning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logger\n",
    "log_dir = Path(\"training\") / \"logs\"\n",
    "# log_every_n_steps = 50\n",
    "# print(log_dir)\n",
    "\n",
    "logger = pl.loggers.WandbLogger(project = \"pytorch-lightning\", log_model=\"all\", job_type=\"train\")\n",
    "\n",
    "# logger = pl.loggers.WandbLogger(name = \"test-callback\", project = \"pytorch-lightning\", log_model=\"all\", save_dir=str(log_dir), job_type=\"train\")\n",
    "\n",
    "# logger.watch(model)\n",
    "# logger.watch(model, log_freq=max(100, log_every_n_steps))\n",
    "\n",
    "# experiment_dir = logger.experiment.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c42c318-cd53-4494-92ab-0efaecdacf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | model     | ViTForImageClassification | 85.8 M\n",
      "1 | train_acc | Accuracy                  | 0     \n",
      "2 | val_acc   | Accuracy                  | 0     \n",
      "3 | test_acc  | Accuracy                  | 0     \n",
      "--------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "171.602   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 9 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 9 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5126fc2134994ac49fa3d235255405cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>ââââ</td></tr><tr><td>global_step</td><td>âââââââââââââââââââ</td></tr><tr><td>train/acc_epoch</td><td>â</td></tr><tr><td>train/acc_step</td><td>ââ</td></tr><tr><td>train/loss_epoch</td><td>â</td></tr><tr><td>train/loss_step</td><td>ââ</td></tr><tr><td>trainer/global_step</td><td>âââââââââââââââââââââ</td></tr><tr><td>validation/acc</td><td>â</td></tr><tr><td>validation/loss</td><td>â</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>global_step</td><td>130</td></tr><tr><td>train/acc_epoch</td><td>0.90522</td></tr><tr><td>train/acc_step</td><td>1.0</td></tr><tr><td>train/loss_epoch</td><td>0.35759</td></tr><tr><td>train/loss_step</td><td>0.04142</td></tr><tr><td>trainer/global_step</td><td>129</td></tr><tr><td>validation/acc</td><td>0.33083</td></tr><tr><td>validation/loss</td><td>3.08626</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">peach-cherry-20</strong>: <a href=\"https://wandb.ai/coffeedrunk/pytorch-lightning/runs/1q96tylh\" target=\"_blank\">https://wandb.ai/coffeedrunk/pytorch-lightning/runs/1q96tylh</a><br/>Synced 5 W&B file(s), 19 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221114_215133-1q96tylh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainer\n",
    "# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer\n",
    "\n",
    "# Notes:\n",
    "# gpus=int(torch.cuda.is_available()) will be removed on pytorch lightning v2.0\n",
    "# The recommendation is to use devices and accelerator instead.\n",
    "\n",
    "# trainer = pl.Trainer(accelerator = 'gpu',\n",
    "#                      devices = int(torch.cuda.is_available()),\n",
    "#                      max_epochs = 5,\n",
    "#                      precision = 32,\n",
    "#                      log_every_n_steps = log_every_n_steps,\n",
    "#                      logger = logger)\n",
    "\n",
    "\n",
    "# grab samples to log predictions on\n",
    "# samples = next(iter(val_dataloader))\n",
    "\n",
    "# devices = int(torch.cuda.is_available())\n",
    "\n",
    "# trainer = pl.Trainer(accelerator = ('gpu' if devices!=0 else \"cpu\"),\n",
    "#                      max_epochs = 1,\n",
    "#                      precision = 16,\n",
    "#                      log_every_n_steps = 50,\n",
    "#                      logger = logger,\n",
    "#                      deterministic=True,     # keep it deterministic\n",
    "#                      callbacks=[ImagePredictionLogger(samples)])\n",
    "\n",
    "devices = int(torch.cuda.is_available())\n",
    "\n",
    "trainer = pl.Trainer(accelerator = ('gpu' if devices!=0 else \"cpu\"),\n",
    "                     devices = devices,\n",
    "                     max_epochs = 1,\n",
    "                     precision = 16,\n",
    "                     log_every_n_steps = 50,\n",
    "                     logger = logger,\n",
    "                     deterministic=True,    # keep it deterministic\n",
    "                     callbacks=[ImagePredictionLogger()])\n",
    "\n",
    "# # test without logger\n",
    "# trainer = pl.Trainer(accelerator = 'gpu',\n",
    "#                      devices = int(torch.cuda.is_available()),\n",
    "#                      max_epochs = 1,\n",
    "#                      precision = 32)\n",
    "\n",
    "\n",
    "trainer.fit(model = model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=val_dataloader)\n",
    "\n",
    "# To change the job status to finished on weights and biases\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ebbc5-efc9-4814-b231-8fb0dff2fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change the job status to finished on weights and biases\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf3092-688b-45f2-b669-3883f7754eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc49974-5119-40ae-95ef-0a9d8c306d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a93101-8fdd-425e-96a8-78554929b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = int(torch.cuda.is_available())\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d59a3-b560-415b-b7d2-20628d92c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed3327-16d1-4df6-8370-c189e853feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b6913-881a-487f-9ec9-da35f5ad0766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineNumbers": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
